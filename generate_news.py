import os
import time
import datetime
import feedparser
import google.generativeai as genai
from google.api_core import exceptions
import groq as groq_lib
import re
import random
from dataclasses import dataclass, field
from functools import lru_cache
from typing import Dict, List
from urllib.parse import urlparse

try:
    from deep_translator import GoogleTranslator
except ImportError:  # pragma: no cover - optional dependency for local dev
    GoogleTranslator = None

if not hasattr(groq_lib, "Groq"):
    raise ImportError(
        "The installed `groq` package does not expose the `Groq` client class. "
        "Install groq>=0.4.2 to enable Groq-based summarization."
    )

Groq = groq_lib.Groq

@dataclass
class CategoryConfig:
    key: str
    display_name: str
    rss_feeds: List[str]
    archive_dir: str
    index_path: str
    max_articles: int = 15
    fallback_image_url: str = ""
    # selection_mode: ê¸°ì‚¬ ì„ íƒ ë°©ì‹ ì„¤ì • (ì›Œí¬í”Œë¡œ ì…ë ¥ â†’ í™˜ê²½ ë³€ìˆ˜ â†’ ê¸°ë³¸ê°’ ìˆœìœ¼ë¡œ ê²°ì •)
    #  - "time": RSSì—ì„œ ê°€ì ¸ì˜¨ ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬í•´ ìƒìœ„ Nê°œ ì„ íƒ
    #  - "random": ì •ë ¬ ì—†ì´ ë¬´ì‘ìœ„ ì„ê¸° í›„ ìƒìœ„ Nê°œ ì„ íƒ
    #  - "keyword": ì§€ì •í•œ í‚¤ì›Œë“œê°€ ì œëª©Â·ë³¸ë¬¸ì— ë“¤ì–´ê°„ ê¸°ì‚¬ë§Œ í•„í„°ë§ í›„ ìµœì‹  ìˆœìœ¼ë¡œ ì„ íƒ
    selection_mode: str = "time"
    keyword_filters: List[str] = field(default_factory=list)
    # importance_scoring: LLMì„ ì´ìš©í•´ ê¸°ì‚¬ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ê³  ì„ ë³„í• ì§€ ì—¬ë¶€
    use_ai_ranking: bool = False


# --- ì„¤ì • ---


def resolve_selection_mode(key: str, default: str = "time") -> str:
    # <ì¹´í…Œê³ ë¦¬>_SELECTION_MODE í™˜ê²½ ë³€ìˆ˜ë¡œ ì„ íƒ ëª¨ë“œë¥¼ ì§€ì •í•œë‹¤.
    # ì˜ˆ) AI ì¹´í…Œê³ ë¦¬ì— ëœë¤ ì ìš©: `export AI_SELECTION_MODE=random`
    # GitHub Actions( daily-news.yml )ì—ì„œ workflow_dispatch ì…ë ¥ì„ ì´ í™˜ê²½ ë³€ìˆ˜ë¡œ ì „ë‹¬í•œë‹¤.
    # ì§€ì› ê°’ ì™¸ê°€ ë“¤ì–´ì˜¤ë©´ ê¸°ë³¸ê°’(default)ì„ ì‚¬ìš©í•œë‹¤.
    env_val = os.getenv(f"{key.upper()}_SELECTION_MODE", default).strip().lower()
    if env_val in {"time", "random", "keyword"}:
        return env_val
    return default


def resolve_use_ai_ranking(key: str, default: bool = False) -> bool:
    # <ì¹´í…Œê³ ë¦¬>_USE_AI_RANKING í™˜ê²½ ë³€ìˆ˜ê°€ "true"ë©´ AI ë­í‚¹ì„ ì ìš©í•œë‹¤.
    env_val = os.getenv(f"{key.upper()}_USE_AI_RANKING", str(default)).lower()
    return env_val == "true"


def resolve_keyword_filters(key: str) -> List[str]:
    # <ì¹´í…Œê³ ë¦¬>_KEYWORDS í™˜ê²½ ë³€ìˆ˜ì— ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œë¥¼ ë„£ëŠ”ë‹¤.
    # ì˜ˆ) AI ì¹´í…Œê³ ë¦¬ì— "openai"ì™€ "llm"ì„ í•„í„°ë§: `export AI_KEYWORDS="openai,llm"`
    # daily-news.ymlì˜ workflow_dispatch ì…ë ¥ì´ ë™ì¼ ì´ë¦„ì˜ í™˜ê²½ ë³€ìˆ˜ë¡œ ì±„ì›Œì ¸ ì—¬ê¸°ì—ì„œ ì½íŒë‹¤.
    # í‚¤ì›Œë“œëŠ” ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì œëª©Â·ë³¸ë¬¸ì—ì„œ ê²€ìƒ‰ëœë‹¤.
    raw = os.getenv(f"{key.upper()}_KEYWORDS", "").strip()
    if not raw:
        return []
    return [kw.strip() for kw in raw.split(",") if kw.strip()]


# --- ì„¤ì • ---
CATEGORIES: Dict[str, CategoryConfig] = {
    "ai": CategoryConfig(
        key="ai",
        display_name="AI",
        rss_feeds=[
            "https://www.aitimes.com/rss/allArticle.xml",
            "https://www.bloter.net/archives/category/ai/feed",
            "https://www.reddit.com/r/ArtificialInteligence/top/.rss?t=day",
            "https://www.techmeme.com/feed.xml",
            # New Feeds
            "https://techcrunch.com/category/artificial-intelligence/feed/",
            "https://a16z.com/feed/",
            "https://www.themiilk.com/feed",
            "https://www.technologyreview.com/feed/",
            "https://www.technologyreview.com/topic/artificial-intelligence/feed/",
            "http://www.aitimes.kr/rss/allArticle.xml",
        ],
        archive_dir="docs/ai/daily",
        index_path="docs/ai/index.html",
        fallback_image_url="https://placehold.co/800x420/111827/FFFFFF?text=AI+News",
        selection_mode=resolve_selection_mode("ai"),
        keyword_filters=resolve_keyword_filters("ai"),
        use_ai_ranking=resolve_use_ai_ranking("ai", default=True),  # Apply AI ranking by default for AI
    ),
    "xr": CategoryConfig(
        key="xr",
        display_name="XR",
        rss_feeds=[
            "https://www.roadtovr.com/feed/",
            "https://uploadvr.com/rss",
            "https://arinsider.co/feed/",
            "https://skarredghost.com/feed/",
        ],
        archive_dir="docs/xr/daily",
        index_path="docs/xr/index.html",
        selection_mode=resolve_selection_mode("xr"),
        keyword_filters=resolve_keyword_filters("xr"),
    ),
}

# Gemini í˜¸ì¶œ ê°„ê²© (ë¬´ë£Œ í”Œëœì´ë©´ 6~7ì´ˆ ì´ìƒ ê¶Œì¥, ìœ ë£Œ/ì—¬ìœ  ìˆìœ¼ë©´ ì¤„ì—¬ë„ ë¨)
REQUEST_INTERVAL_SECONDS = 2
# Google ì¸¡ì—ì„œ ì œì•ˆí•˜ëŠ” retry-after ê°’ì´ 1ë¶„ ì´ìƒì¼ ë•Œ ì›Œí¬í”Œë¡œê°€ ë©ˆì¶° ë³´ì´ëŠ” ê²ƒì„
# ë°©ì§€í•˜ê¸° ìœ„í•´ ì§€ì—° ì‹œê°„ì„ ìƒí•œ ì²˜ë¦¬í•œë‹¤.
MAX_GEMINI_RETRY_DELAY = 15.0
HIGHLIGHT_COLOR = "#fff6b0"


# **í…ìŠ¤íŠ¸** â†’ ê°•ì¡° ìƒ‰ìƒ(ë¬¸êµ¬ë§Œ) + ëª©ë¡ ì²˜ë¦¬
def markdown_bold_to_highlight(html_text: str) -> str:
    """Convert **bold** markers into highlighted phrases and list items."""

    def wrap_highlight(match):
        text = match.group(1)
        if len(text.split()) >= 2:
            return (
                f"<span class='highlight' style='background-color: {HIGHLIGHT_COLOR};"
                " padding: 3px 5px; border-radius: 4px;'>"
                f"{text}"
                "</span>"
            )
        return f"<strong>{text}</strong>"

    lines = []
    for raw_line in html_text.splitlines():
        cleaned = raw_line.strip()
        if not cleaned:
            continue
        cleaned = re.sub(r"^[â€¢â–¡\-]\s*", "", cleaned)
        converted = re.sub(r"\*\*(.+?)\*\*", wrap_highlight, cleaned)
        lines.append(converted)

    if not lines:
        return ""

    items = [f"<li>{line}</li>" for line in lines]
    return "<ul class='summary-list'>" + "".join(items) + "</ul>"


def contains_korean(text: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", text))


_translator = None


@lru_cache(maxsize=256)
def translate_title_to_korean(title: str) -> str:
    """Translate English titles to Korean for display. Fallback to original on failure."""

    if not title or contains_korean(title):
        return title

    if GoogleTranslator is None:
        return title

    global _translator
    if _translator is None:
        _translator = GoogleTranslator(source="auto", target="ko")

    try:
        result = _translator.translate(title)
        if result:
            return result
    except Exception:
        pass

    return title


def format_timestamp(ts: float) -> str:
    if not ts:
        return "ë°œí–‰ ì‹œê° ì •ë³´ ì—†ìŒ"

    try:
        return datetime.datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M")
    except Exception:
        return "ë°œí–‰ ì‹œê° ì •ë³´ ì—†ìŒ"


def extract_source_name(entry, link: str) -> str:
    source_title = getattr(entry, "source", None)
    if source_title:
        title_val = getattr(source_title, "title", None)
        if title_val:
            return title_val

    netloc = urlparse(link or "").netloc
    if netloc.startswith("www."):
        netloc = netloc[4:]
    return netloc or "ì¶œì²˜ ë¯¸ìƒ"


def extract_image_url(entry) -> str:
    media_content = getattr(entry, "media_content", None) or []
    if media_content:
        first = media_content[0]
        if isinstance(first, dict) and first.get("url"):
            return first["url"]

    media_thumbnail = getattr(entry, "media_thumbnail", None) or []
    if media_thumbnail:
        thumb = media_thumbnail[0]
        if isinstance(thumb, dict) and thumb.get("url"):
            return thumb["url"]

    image_link = getattr(entry, "image", None)
    if isinstance(image_link, dict) and image_link.get("href"):
        return image_link["href"]

    def extract_from_html(html_text: str) -> str:
        if not html_text:
            return ""
        match = re.search(r"<img[^>]+src=['\"]([^'\"]+)['\"]", html_text, re.IGNORECASE)
        return match.group(1) if match else ""

    contents = getattr(entry, "content", None) or []
    for content in contents:
        if isinstance(content, dict):
            candidate = extract_from_html(content.get("value", ""))
        else:
            candidate = extract_from_html(getattr(content, "value", ""))
        if candidate:
            return candidate

    summary_html = getattr(entry, "summary", "") or getattr(entry, "description", "")
    html_candidate = extract_from_html(summary_html)
    if html_candidate:
        return html_candidate

    return ""


def sanitize_summary(summary: str) -> str:
    cleaned_lines = []
    for line in summary.splitlines():
        stripped = line.strip()
        if not stripped:
            continue
        if "URL:" in stripped:
            continue
        if re.search(r"ì¶œì²˜\s*:", stripped):
            continue
        if re.search(r"https?://", stripped):
            continue
        cleaned_lines.append(stripped)

    return "\n".join(cleaned_lines)


def _extract_retry_delay(exc: Exception, default: float = 30.0) -> float:
    """Extract retry-after seconds from Gemini quota errors."""

    message = str(exc).lower()
    match = re.search(r"retry in ([0-9]+(?:\.[0-9]+)?)s", message)
    if match:
        try:
            return min(float(match.group(1)), MAX_GEMINI_RETRY_DELAY)
        except ValueError:
            pass

    return min(default, MAX_GEMINI_RETRY_DELAY)


# 1) Gemini/Grok ìš”ì•½ í•¨ìˆ˜
def _build_summary_prompt(text: str, title: str, display_name: str) -> str:
    return f"""
ì•„ë˜ {display_name} ê´€ë ¨ ê¸°ì‚¬ ë‚´ìš©ì„ 5ì¤„ ì´ë‚´ í•œêµ­ì–´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜.
ê°€ëŠ¥í•˜ë©´ ìˆ˜ì¹˜, íšŒì‚¬ëª…, í•µì‹¬ ì´ìŠˆ ìœ„ì£¼ë¡œ í•˜ê³ , ê° ì¤„ì€ ë¶ˆë¦¿ ê¸°í˜¸ "â–¡"ìœ¼ë¡œ ì‹œì‘í•´ì¤˜.
í•µì‹¬ í‚¤ì›Œë“œëŠ” ê°•ì¡°(**êµµê²Œ**) ì²˜ë¦¬í•˜ë˜, URLì´ë‚˜ ë§í¬ëŠ” í¬í•¨í•˜ì§€ ë§ˆ.

ì œëª©: {title}
ë‚´ìš©:
{text[:2000]}
"""


def _summarize_with_gemini(prompt: str) -> str:
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])

    # ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ (í•„ìš”ì‹œ ë‹¤ë¥¸ IDë¡œ êµì²´)
    model = genai.GenerativeModel("gemini-2.5-flash-preview-09-2025")

    last_exc: Exception | None = None
    for attempt in range(3):
        try:
            res = model.generate_content(prompt)
            return res.text.strip()
        except exceptions.ResourceExhausted as exc:
            last_exc = exc
            if attempt == 2:
                raise

            delay = _extract_retry_delay(exc)
            print(
                f"[warn] Gemini quota exceeded, retrying in {delay:.1f}s "
                f"(attempt {attempt + 2}/3)"
            )
            time.sleep(delay)
        except exceptions.GoogleAPICallError as exc:
            last_exc = exc
            if attempt == 2:
                raise

            backoff = (attempt + 1) * 5
            print(
                f"[warn] Gemini API error ({exc}); retrying in {backoff}s "
                f"(attempt {attempt + 2}/3)"
            )
            time.sleep(backoff)

    raise last_exc if last_exc else RuntimeError("Gemini summarization failed")


def _summarize_with_grok(prompt: str) -> str:
    api_key = os.getenv("GROK_API_KEY")
    if not api_key:
        raise RuntimeError("GROK_API_KEY is not set for Grok fallback")

    client = Groq(api_key=api_key)
    model = os.getenv("GROK_MODEL", "llama-3.3-70b-versatile")

    res = client.chat.completions.create(
        messages=[{"role": "user", "content": prompt}],
        model=model,
    )

    content = res.choices[0].message.content if res.choices else ""
    if not content:
        raise RuntimeError("Grok did not return a summary")

    return content.strip()


def summarize(text: str, title: str, display_name: str) -> str:
    prompt = _build_summary_prompt(text, title, display_name)

    try:
        return _summarize_with_grok(prompt)
    except Exception as grok_exc:
        print(f"[warn] Grok failed with error: {grok_exc}; falling back to Gemini")

        try:
            return _summarize_with_gemini(prompt)
        except Exception as gemini_exc:
            raise RuntimeError(
                "Both Grok and Gemini summarization failed. Check API keys and quotas."
            ) from gemini_exc





# 1.5) AI ì¤‘ìš”ë„ í‰ê°€ (Ranking)
def _build_ranking_prompt(items_for_ranking: List[tuple], limit: int) -> str:
    # (idx, title) ëª©ë¡ ìƒì„±
    candidates = []
    for idx, (ts, title, link, content, entry) in enumerate(items_for_ranking):
        candidates.append(f"{idx}. {title}")
    
    candidates_text = "\n".join(candidates)

    return f"""
ë‹¤ìŒì€ ë‹¤ì–‘í•œ AI/í…Œí¬ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ì œëª© ë¦¬ìŠ¤íŠ¸ì•¼.
ì´ ì¤‘ì—ì„œ ì˜¤ëŠ˜ë‚ ì§œ ë‰´ìŠ¤ë ˆí„°ì— í¬í•¨ì‹œí‚¬ ê°€ì¥ 'ì¤‘ìš”í•˜ê³  ì˜ë¯¸ ìˆëŠ”' ê¸°ì‚¬ {limit}ê°œë¥¼ ê³¨ë¼ì¤˜.

ì¤‘ìš”ë„ íŒë‹¨ ê¸°ì¤€:
1. ì£¼ìš” ê¸°ìˆ  ê¸°ì—…(OpenAI, Google, Apple ë“±)ì˜ ìƒˆë¡œìš´ ì œí’ˆ/ëª¨ë¸ ì¶œì‹œ
2. AI ë¶„ì•¼ì˜ íšê¸°ì ì¸ ì—°êµ¬ ì„±ê³¼ë‚˜ ë…¼ë¬¸
3. ì—…ê³„ì˜ í° ì¸ìˆ˜í•©ë³‘ì´ë‚˜ ì •ì±… ë³€í™”
4. ë‹¨ìˆœ íŠœí† ë¦¬ì–¼ì´ë‚˜ í™ë³´ì„± ê¸°ì‚¬ëŠ” ì œì™¸

ì‘ë‹µ í˜•ì‹:
- ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°ë˜ëŠ” ê¸°ì‚¬ì˜ 'ì¸ë±ìŠ¤ ë²ˆí˜¸'ë¥¼ ì¤‘ìš”ë„ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•´ì¤˜.
- ë²ˆí˜¸ ì™¸ì— ë‹¤ë¥¸ ì„¤ëª…ì€ í•„ìš” ì—†ì–´.
- ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ì¤˜. ì˜ˆ: 1, 5, 10, 3, 2

[ê¸°ì‚¬ ëª©ë¡]
{candidates_text}
"""

def rank_items_with_ai(raw_items: List[tuple], limit: int) -> List[tuple]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ê³  ìƒìœ„ limitê°œë¥¼ ë°˜í™˜í•œë‹¤.
    - ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ raw_itemsê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìµœì‹  50ê°œ ì •ë„ë¡œ ë¨¼ì € ìë¥´ê³  ë³´ë‚¸ë‹¤.
    """
    if not raw_items:
        return []

    # API ë¹„ìš© ê³ ë ¤: ìµœì‹ ìˆœ ì •ë ¬ í›„ ìµœëŒ€ 60ê°œë§Œ LLMì—ê²Œ í‰ê°€ ìš”ì²­ (ê·¸ ì´ìƒì€ í† í°/ë¹„ìš© ë‚­ë¹„ ê°€ëŠ¥ì„±)
    candidates = sorted(raw_items, key=lambda x: x[0], reverse=True)[:60]
    
    prompt = _build_ranking_prompt(candidates, limit)
    
    ranked_indices = []
    try:
        # ìš”ì•½ê³¼ ë™ì¼í•œ Grok -> Gemini ë¡œì§ ì‚¬ìš©
        response_text = _summarize_with_grok(prompt)
        # ì‘ë‹µ íŒŒì‹± (ìˆ«ìë§Œ ì¶”ì¶œ)
        # ì˜ˆ: "1, 5, 23" -> [1, 5, 23]
        matches = re.findall(r"\d+", response_text)
        ranked_indices = [int(m) for m in matches]
        
    except Exception as e:
        print(f"[warn] AI ranking failed ({e}); falling back to time-based sorting.")
        return candidates[:limit]

    # ì„ íƒëœ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” í•­ëª© ì¶”ì¶œ (ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€)
    selected_items = []
    seen_indices = set()
    
    # LLMì´ ë°˜í™˜í•œ ìˆœì„œëŒ€ë¡œ ë‹´ê¸°
    for idx in ranked_indices:
        if idx in seen_indices:
            continue
        if 0 <= idx < len(candidates):
            selected_items.append(candidates[idx])
            seen_indices.add(idx)
            
    # ë§Œì•½ LLMì´ ì¶©ë¶„í•œ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ì›ë³¸ ìˆœì„œ(ìµœì‹ ìˆœ)ëŒ€ë¡œ ë‚˜ë¨¸ì§€ ì±„ì›€
    if len(selected_items) < limit:
        for idx in range(len(candidates)):
            if idx not in seen_indices:
                selected_items.append(candidates[idx])
                if len(selected_items) >= limit:
                    break
                    
    return selected_items[:limit]


# 2) RSS â†’ (ì—¬ëŸ¬ RSS ì „ì²´) â†’ ì‹œê°„/ëœë¤/AI ì •ë ¬ â†’ ìƒìœ„ Nê°œë§Œ ìš”ì•½
def fetch_and_summarize(config: CategoryConfig):
    raw_items = []

    for feed_url in config.rss_feeds:
        d = feedparser.parse(feed_url)

        for entry in d.entries:
            title = getattr(entry, "title", "")
            link = getattr(entry, "link", "")
            content = getattr(entry, "summary", "") or getattr(
                entry, "description", ""
            )

            # ê²Œì‹œ ì‹œê°(published) ë˜ëŠ” ìˆ˜ì • ì‹œê°(updated) ì‚¬ìš©
            published = getattr(entry, "published_parsed", None) or getattr(
                entry, "updated_parsed", None
            )
            if published:
                ts = time.mktime(published)  # epoch time
            else:
                ts = 0  # ë‚ ì§œ ì •ë³´ ì—†ìœ¼ë©´ ê°€ì¥ ë’¤ë¡œ ë°€ë¦¼ (time ëª¨ë“œì¼ ë•Œ)

            raw_items.append((ts, title, link, content, entry))

    keywords = [kw.lower() for kw in config.keyword_filters]

    if config.selection_mode == "keyword" and keywords:
        # í‚¤ì›Œë“œ ëª¨ë“œ: ì œëª©+ë³¸ë¬¸ì— í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë‚¨ê¸´ë‹¤.
        # í‚¤ì›Œë“œëŠ” resolve_keyword_filters()ë¡œ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜¨ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤.
        raw_items = [
            item
            for item in raw_items
            if any(kw in ((item[1] or "") + " " + (item[3] or "")).lower() for kw in keywords)
        ]

    # ê¸°ì‚¬ ì •ë ¬/ì„ íƒ ë°©ì‹
    three_days_ago = time.time() - 3 * 24 * 60 * 60
    if config.selection_mode == "time":
        # ì‹œê°„ ëª¨ë“œ: ê²Œì‹œ ì‹œê°(ts) ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ìµœì‹  ê¸°ì‚¬ë¶€í„° ì •ë ¬
        raw_items.sort(key=lambda x: x[0], reverse=True)
    elif config.selection_mode == "random":
        # ëœë¤ ëª¨ë“œ: ìµœê·¼ 3ì¼ ë‚´ ê¸°ì‚¬ë§Œ ëŒ€ìƒìœ¼ë¡œ ë¬´ì‘ìœ„ ì„ê¸° (ì—†ìœ¼ë©´ ì „ì²´ ì‚¬ìš©)
        recent_items = [item for item in raw_items if item[0] and item[0] >= three_days_ago]
        candidate_items = recent_items if recent_items else raw_items
        random.shuffle(candidate_items)
        raw_items = candidate_items
    elif config.selection_mode == "keyword":
        # í‚¤ì›Œë“œ ëª¨ë“œ: í•„í„°ë§ í›„ ìµœì‹  ìˆœ ì •ë ¬ (í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì•„ë˜ elseë¡œ ë™ì¼ ì²˜ë¦¬)
        raw_items.sort(key=lambda x: x[0], reverse=True)
    else:
        raw_items.sort(key=lambda x: x[0], reverse=True)

    # ---------------------------------------------------------
    # AI ë­í‚¹ ì ìš© ì—¬ë¶€ í™•ì¸
    # ---------------------------------------------------------
    # selection_modeê°€ 'random'ì´ ì•„ë‹ˆê³ , configì— use_ai_rankingê°€ ì¼œì ¸ ìˆìœ¼ë©´
    # ê¸°ì¡´ ì •ë ¬ ê²°ê³¼ì—ì„œ AIê°€ ë‹¤ì‹œ ì„ ë³„í•œë‹¤.
    if config.use_ai_ranking and config.selection_mode != "random":
        print(f"[{config.key.upper()}] Applying AI Ranking for selection...")
        selected = rank_items_with_ai(raw_items, config.max_articles)
    else:
        # ìƒìœ„ Nê°œë§Œ ì„ íƒ (ê¸°ì¡´ ë°©ì‹)
        selected = raw_items[: config.max_articles]

    summarized = []
    for idx, (ts, title, link, content, entry) in enumerate(selected):
        text_with_url = content + f"\n\nê¸°ì‚¬ URL: {link}"

        summary = summarize(text_with_url, title, config.display_name)
        summary = sanitize_summary(summary)
        image_url = extract_image_url(entry) or config.fallback_image_url

        summarized.append(
            {
                "title": title,
                "link": link,
                "summary": summary,
                "published_display": format_timestamp(ts),
                "source_name": extract_source_name(entry, link),
                "image_url": image_url,
            }
        )

        # ì¿¼í„° ë³´í˜¸ìš© ë”œë ˆì´
        time.sleep(REQUEST_INTERVAL_SECONDS)

    return summarized


# 3) ê°œë³„ ì‹¤í–‰(ë‚ ì§œ+ì‹œê°„) í˜ì´ì§€ ìƒì„±
def build_daily_page(articles, date_str: str, time_str: str, config: CategoryConfig) -> str:
    parts = []
    parts.append("<!DOCTYPE html>")
    parts.append("<html lang='ko'>")
    parts.append("<head>")
    parts.append("  <meta charset='utf-8' />")
    parts.append(f"  <title>{config.display_name} News - {date_str} {time_str}</title>")
    parts.append(
        "  <meta name='viewport' content='width=device-width, initial-scale=1' />"
    )
    parts.append(
        "    body { font-family: Roboto, 'Noto Sans KR', 'Pretendard', sans-serif; line-height: 1.5; margin: 0; background: #fff; color: #111; }"
    )
    parts.append("    a { text-decoration: none; color: inherit; }")
    parts.append("    .container { max-width: 1000px; margin: 0 auto; padding: 20px; }")
    parts.append("    h1 { font-size: 1.5rem; margin-bottom: 5px; color: #111; letter-spacing: -1px; }")
    parts.append("    .meta { color: #888; font-size: 0.9rem; margin-bottom: 20px; }")
    
    parts.append(
        "    .nav { display: flex; gap: 10px; margin-bottom: 20px; border-bottom: 1px solid #ddd; padding-bottom: 15px; }"
    )
    parts.append(
        "    .nav a { padding: 8px 15px; background: #f4f4f4; border-radius: 4px; color: #333; font-size: 0.9rem; font-weight: bold; transition: background 0.2s; }"
    )
    parts.append("    .nav a:hover { background: #e0e0e0; }")

    parts.append("    .board-list { border-top: 2px solid #2272c9; }")
    parts.append(
        "    .list-item { display: block; border-bottom: 1px solid #e0e0e0; padding: 15px 10px; transition: background 0.2s; }"
    )
    parts.append("    .list-item:hover { background: #f9f9f9; }")
    
    parts.append("    .item-title { font-size: 1.1rem; font-weight: normal; margin: 0 0 5px; line-height: 1.4; }")
    parts.append("    .item-title a { color: #232f3e; transition: color 0.2s; }")
    parts.append("    .item-title a:hover { color: #d43f3a; text-decoration: underline; }")
    
    parts.append(
        "    .item-meta { font-size: 0.85rem; color: #959595; display: flex; gap: 8px; align-items: center; margin-top: 4px; }"
    )
    parts.append("    .source-badge { color: #2272c9; font-weight: bold; }")
    parts.append("    .original-title { color: #aaa; font-size: 0.8rem; display: block; margin-top: 2px; }")

    parts.append("    .item-body { margin-top: 10px; display: flex; gap: 15px; }")
    parts.append(
        "    .summary-text { flex: 1; font-size: 0.95rem; color: #333; line-height: 1.6; word-break: break-all; }"
    )
    parts.append("    .summary-list { margin: 0; padding-left: 1.2rem; }")
    parts.append("    .summary-list li { margin-bottom: 3px; }")
    
    parts.append(
        "    .item-image { width: 120px; height: 90px; object-fit: cover; border-radius: 4px; border: 1px solid #eee; flex-shrink: 0; }"
    )
    
    parts.append(
        "    .highlight { background-color: #fff8c4; padding: 2px 4px; border-radius: 2px; }"
    )
    
    parts.append("    @media (max-width: 600px) {")
    parts.append("      .container { padding: 15px; }")
    parts.append("      .item-image { width: 80px; height: 60px; }")
    parts.append("    }")
    parts.append("  </style>")
    parts.append("</head>")
    parts.append("<body>")
    parts.append("  <div class='container'>")
    parts.append("    <div class='nav'>")
    parts.append("      <a href='../../index.html'>ğŸ  í™ˆìœ¼ë¡œ</a>")
    parts.append("      <a href='../index.html'>ğŸ“… ë‚ ì§œë³„ ëª©ë¡</a>")
    parts.append("    </div>")
    parts.append(f"    <h1>{date_str} {config.display_name} News</h1>")
    parts.append(f"    <p class='meta'>Updated at {time_str} (KST)</p>")
    parts.append("    <div class='board-list'>")

    for art in articles:
        summary_html = markdown_bold_to_highlight(art["summary"])
        display_title = translate_title_to_korean(art["title"])
        original_hint = (
            f"<span class='original-title'>ì›ë¬¸ ì œëª©: {art['title']}</span>"
            if display_title != art["title"]
            else ""
        )

        parts.append("      <div class='list-item'>")
        parts.append(
            "        <div class='item-title'>"
            f"<a href='{art['link']}' target='_blank'>{display_title}</a>"
        )
        if original_hint:
             parts.append(f"          <br/>{original_hint}")
        parts.append("        </div>")

        meta_bits = [bit for bit in [art.get("source_name"), art.get("published_display")] if bit]
        if meta_bits:
            parts.append("        <div class='item-meta'>")
            parts.append(f"          <span class='source-badge'>{meta_bits[0]}</span>")
            if len(meta_bits) > 1:
                parts.append(f"          <span>| {meta_bits[1]}</span>")
            parts.append("        </div>")

        if summary_html or art.get("image_url"):
            parts.append("        <div class='item-body'>")
            
            if art.get("image_url"):
                parts.append(
                    f"          <img src='{art['image_url']}' alt='ê¸°ì‚¬ ì´ë¯¸ì§€' class='item-image' loading='lazy'/>"
                )

            parts.append("          <div class='summary-text'>")
            if summary_html:
                parts.append(f"            {summary_html}")
            parts.append("          </div>")
            parts.append("        </div>")
            
        parts.append("      </div>")

    parts.append("    </div>") # End board-list
    parts.append("  </div>") # End container
    parts.append("</body>")
    parts.append("</html>")

    return "\n".join(parts)


# 4) index.html ëª©ë¡ í˜ì´ì§€ ì¬ìƒì„± (ì—¬ëŸ¬ ë²ˆ/í•˜ë£¨ ì—¬ëŸ¬ íšŒ ì‹¤í–‰ í¬í•¨)
def collect_run_entries(config: CategoryConfig):
    os.makedirs(config.archive_dir, exist_ok=True)

    files = [f for f in os.listdir(config.archive_dir) if f.endswith(".html")]

    run_entries = []
    for fname in files:
        base = fname.replace(".html", "")
        date_str = base
        time_str = ""

        if "_" in base:
            date_part, time_part = base.split("_", 1)
            date_str = date_part
            if len(time_part) >= 6:
                hh = time_part[0:2]
                mm = time_part[2:4]
                ss = time_part[4:6]
                time_str = f"{hh}:{mm}:{ss}"
            else:
                time_str = time_part
        run_entries.append((base, date_str, time_str, fname))

    run_entries.sort(key=lambda x: x[0], reverse=True)
    return run_entries


def rebuild_index_html(config: CategoryConfig):
    run_entries = collect_run_entries(config)

    parts = []
    parts.append("<!DOCTYPE html>")
    parts.append("<html lang='ko'>")
    parts.append("<head>")
    parts.append("  <meta charset='utf-8' />")
    parts.append(f"  <title>Daily {config.display_name} News Archive</title>")
    parts.append(
        "  <meta name='viewport' content='width=device-width, initial-scale=1' />"
    )
    parts.append("  <style>")
    parts.append(
        "    body { font-family: Roboto, 'Noto Sans KR', 'Pretendard', sans-serif; margin: 0; line-height: 1.6; background: #fff; color: #111; }"
    )
    parts.append("    a { text-decoration: none; color: inherit; }")
    parts.append("    .container { max-width: 1000px; margin: 0 auto; padding: 20px; }")
    
    parts.append("    h1 { margin-bottom: 0.35rem; font-size: 1.5rem; letter-spacing: -1px; }")
    parts.append(
        "    .nav { display: flex; gap: 10px; margin-bottom: 20px; border-bottom: 1px solid #ddd; padding-bottom: 15px; }"
    )
    parts.append(
        "    .nav a { padding: 8px 15px; background: #f4f4f4; border-radius: 4px; color: #333; font-size: 0.9rem; font-weight: bold; transition: background 0.2s; }"
    )
    parts.append("    .nav a:hover { background: #e0e0e0; }")

    parts.append("    .run-list { list-style: none; padding: 0; border-top: 2px solid #2272c9; margin-top: 15px; }")
    parts.append(
        "    .run-item { border-bottom: 1px solid #e0e0e0; padding: 12px 10px; transition: background 0.2s; }"
    )
    parts.append("    .run-item:hover { background: #f9f9f9; }")
    
    parts.append(
        "    .run-item a { color: #232f3e; font-size: 1.1rem; }"
    )
    parts.append("    .run-item a:hover { color: #d43f3a; text-decoration: underline; }")
    parts.append(
        "    .timestamp { color: #888; font-size: 0.85rem; display: block; margin-top: 4px; }"
    )
    parts.append("  </style>")
    parts.append("</head>")
    parts.append("<body>")
    parts.append("  <div class='container'>")
    parts.append("    <div class='nav'>")
    parts.append("      <a href='../index.html'>ğŸ  í™ˆìœ¼ë¡œ</a>")
    parts.append("    </div>")
    parts.append(f"    <h1>Daily {config.display_name} News Archive</h1>")
    parts.append(
        f"  <p>ì‹¤í–‰ ì‹œì (ë‚ ì§œ+ì‹œê°„, KST)ë³„ë¡œ ì €ì¥ëœ {config.display_name} ê¸°ì‚¬ ìš”ì•½ ëª©ë¡ì…ë‹ˆë‹¤.</p>"
    )

    if not run_entries:
        parts.append("  <p>ì•„ì§ ì €ì¥ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.</p>")
    else:
        parts.append("  <ul class='run-list'>")
        for base, date_str, time_str, fname in run_entries:
            if time_str:
                label = f"{date_str} {time_str} {config.display_name} News"
            else:
                label = f"{date_str} {config.display_name} News"
            parts.append(
                "    <li class='run-item'>"
                f"<a href='daily/{fname}'>{label}</a>"
                f"<span class='timestamp'>ì›ë³¸ ìƒì„± ì‹œê°„: {date_str} {time_str or ''} (KST)</span>"
                "</li>"
            )
        parts.append("  </ul>")

    parts.append("  </div>")
    parts.append("</body>")
    parts.append("</html>")

    os.makedirs(os.path.dirname(config.index_path), exist_ok=True)
    with open(config.index_path, "w", encoding="utf-8") as f:
        f.write("\n".join(parts))


def build_root_index(categories: Dict[str, CategoryConfig]):
    categorized_runs = {cfg.key: collect_run_entries(cfg) for cfg in categories.values()}

    parts = []
    parts.append("<!DOCTYPE html>")
    parts.append("<html lang='ko'>")
    parts.append("<head>")
    parts.append("  <meta charset='utf-8' />")
    parts.append("  <title>AI & XR News Archives</title>")
    parts.append("  <meta name='viewport' content='width=device-width, initial-scale=1' />")
    parts.append("  <style>")
    parts.append(
        "    body { font-family: Roboto, 'Noto Sans KR', 'Pretendard', sans-serif; margin: 0; line-height: 1.6; background: #fff; color: #111; }"
    )
    parts.append("    a { text-decoration: none; color: inherit; }")
    parts.append("    .container { max-width: 1000px; margin: 0 auto; padding: 20px; }")

    parts.append("    h1 { margin-bottom: 0.5rem; letter-spacing: -1px; }")
    parts.append("    .subtitle { color: #555; margin-bottom: 20px; font-size: 0.95rem; }")
    
    parts.append("    .tabs { display: flex; gap: 5px; margin-bottom: 15px; border-bottom: 2px solid #2272c9; }")
    parts.append(
        "    .tab-btn { padding: 10px 20px; border: 1px solid #ddd; border-bottom: none; border-radius: 5px 5px 0 0; background: #f9f9f9; cursor: pointer; font-weight: bold; color: #555; margin-bottom: -2px; z-index: 1; }"
    )
    parts.append(
        "    .tab-btn.active { background: #2272c9; color: #fff; border-color: #2272c9; }"
    )
    
    parts.append("    .tab-panel { display: none; padding-top: 10px; }")
    parts.append("    .tab-panel.active { display: block; }")
    
    parts.append("    .panel-card { background: #fff; }")
    parts.append("    .list-header { border-bottom: 1px solid #333; padding-bottom: 10px; margin-bottom: 10px; font-weight: bold; }")

    parts.append("    ul { list-style: none; padding: 0; }")
    parts.append("    li { border-bottom: 1px solid #eee; padding: 8px 5px; font-size: 1rem; }")
    parts.append("    li:hover { background: #f9f9f9; }")
    
    parts.append("    .timestamp { color: #888; font-size: 0.85rem; margin-left: 8px; }")
    parts.append("    .archive-link { margin-bottom: 15px; font-weight: bold; }")
    parts.append("    .archive-link a { color: #2272c9; text-decoration: underline; }")
    parts.append("  </style>")
    parts.append("</head>")
    parts.append("<body>")
    parts.append("  <div class='container'>")
    parts.append("    <h1>AI & XR Daily News Archives</h1>")
    parts.append("    <p class='subtitle'>íƒ­ì„ ëˆŒëŸ¬ AI/XR ë‰´ìŠ¤ë¥¼ êµ¬ë¶„í•´ í™•ì¸í•˜ì„¸ìš”. ëª¨ë“  ì‹œê°ì€ í•œêµ­ í‘œì¤€ì‹œ(KST) ê¸°ì¤€ì´ë©°, ê³¼ê±° ì‹¤í–‰ ê²°ê³¼ë„ ëˆ„ì í•´ ë³´ì—¬ì¤ë‹ˆë‹¤.</p>")

    parts.append("  <div class='tabs'>")
    for cfg in categories.values():
        parts.append(
            f"    <button class='tab-btn' data-target='{cfg.key}'>{cfg.display_name} ë‰´ìŠ¤</button>"
        )
    parts.append("  </div>")

    for cfg in categories.values():
        runs = categorized_runs.get(cfg.key, [])
        parts.append(
            f"  <div class='tab-panel' id='{cfg.key}'>"
        )
        parts.append("    <div class='panel-card'>")
        parts.append(
            f"      <p class='archive-link'><a href='{cfg.key}/index.html'>{cfg.display_name} ì•„ì¹´ì´ë¸Œ ì „ì²´ ë³´ê¸° â†’</a></p>"
        )

        if not runs:
            parts.append("      <p>ì•„ì§ ì €ì¥ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.</p>")
        else:
            parts.append("      <ul>")
            for base, date_str, time_str, fname in runs:
                label = f"{date_str} {time_str} KST" if time_str else f"{date_str} KST"
                parts.append(
                    "        <li>"
                    f"<a href='{cfg.key}/daily/{fname}'>{cfg.display_name} ë‰´ìŠ¤</a>"
                    f" <span class='timestamp'>{label}</span>"
                    "</li>"
                )
            parts.append("      </ul>")

        parts.append("    </div>")
        parts.append("  </div>")

    parts.append("  <script>")
    parts.append(
        "    const tabs = document.querySelectorAll('.tab-btn'); const panels = document.querySelectorAll('.tab-panel');"
    )
    parts.append(
        "    function activateTab(key) { panels.forEach(p => p.classList.toggle('active', p.id === key)); tabs.forEach(t => t.classList.toggle('active', t.dataset.target === key)); }"
    )
    parts.append(
        "    tabs.forEach(btn => btn.addEventListener('click', () => activateTab(btn.dataset.target)));"
    )
    parts.append("    if (tabs.length) { activateTab(tabs[0].dataset.target); }")
    parts.append("  </script>")
    parts.append("  </div>")
    parts.append("</body>")
    parts.append("</html>")

    os.makedirs("docs", exist_ok=True)
    with open("docs/index.html", "w", encoding="utf-8") as f:
        f.write("\n".join(parts))


# 5) main ì‹¤í–‰ í•¨ìˆ˜
def main():
    # GitHub ActionsëŠ” UTCì´ë¯€ë¡œ, UTC + 9ì‹œê°„ = KST
    now_utc = datetime.datetime.utcnow()
    now = now_utc + datetime.timedelta(hours=9)

    date_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H:%M:%S")

    # íŒŒì¼ ì´ë¦„ìš© ID (YYYY-MM-DD_HHMMSS, KST ê¸°ì¤€)
    run_id = now.strftime("%Y-%m-%d_%H%M%S")

    for cfg in CATEGORIES.values():
        articles = fetch_and_summarize(cfg)
        daily_html = build_daily_page(articles, date_str, time_str, cfg)

        os.makedirs(cfg.archive_dir, exist_ok=True)
        daily_path = os.path.join(cfg.archive_dir, f"{run_id}.html")

        # ë§¤ ì‹¤í–‰ë§ˆë‹¤ ìƒˆë¡œìš´ íŒŒì¼ ìƒì„±
        with open(daily_path, "w", encoding="utf-8") as f:
            f.write(daily_html)

        rebuild_index_html(cfg)

    build_root_index(CATEGORIES)


if __name__ == "__main__":
    main()
